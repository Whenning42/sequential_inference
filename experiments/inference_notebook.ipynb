{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/Workspaces/sequential_inference/llm-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Measure GPT-2 medium's (350M) performance on the SeqInfer task.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "import synth_gen.gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fp32\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    print(\"Using bf16\")\n",
    "    model = model.to(dtype=torch.bfloat16)\n",
    "else:\n",
    "    print(\"Using fp32\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 [' 2', '2', 'me bunch']\n",
      " 2 [' 2', '2', 'me bunch']\n",
      " bunch [' 2', '2', 'me bunch']\n",
      " tame bunch [' 2', '2', 'me bunch']\n",
      "tensor([[False, False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test me!\n",
    "def right_aligned_covering(suffix: list[int], full: list[int], pad_token: int):\n",
    "    # Get all the unchanged tokens\n",
    "    covering = torch.logical_and(suffix == full, suffix != pad_token)\n",
    "\n",
    "    # Now find all of the tokens in full that contain uncovered for suffix tokens.\n",
    "    B = full.shape[0]\n",
    "    N = full.shape[1]\n",
    "    for b in range(B):\n",
    "        for i in range(N-1, -1, -1):\n",
    "            if covering[b, i]:\n",
    "                break\n",
    "        \n",
    "        split = i\n",
    "        uncovered_tokens = []\n",
    "        while suffix[b, i] != pad_token and i >= 0:\n",
    "            uncovered_tokens.prepend(suffix[b, i])\n",
    "            i -= 1\n",
    "        uncovered_str = tokenizer.decode(uncovered_tokens)\n",
    "\n",
    "        i = split\n",
    "        tail = len(uncovered_str)\n",
    "        covering_string = \"\"\n",
    "        while covering_string[-tail:] != uncovered_str:\n",
    "            print(\"Covering:\", covering_string, \"Uncovered:\", uncovered_str)\n",
    "            covering_string = tokenizer.decode(full[b, i]) + covering_string\n",
    "            covering[b, i] = True\n",
    "            i -= 1\n",
    "    return covering\n",
    "\n",
    "def right_aligned_covering2(full: torch.Tensor, suffix_strs: str, pad_token_id: int):\n",
    "    B = full.shape[0]\n",
    "    mask = torch.zeros_like(full, dtype=torch.bool)\n",
    "    for b in range(B):\n",
    "        n = full.shape[1]\n",
    "        suffix_len = len(suffix_strs[b])\n",
    "        taken = \"\"\n",
    "        for i in range(n-1, -1, -1):\n",
    "            taken = tokenizer.decode(full[b, i]) + taken\n",
    "            print(taken, suffix_strs)\n",
    "            if taken[-suffix_len:] == suffix_strs[b] or full[b, i] == pad_token_id:\n",
    "                break\n",
    "        mask[b, i:] = True\n",
    "    return mask\n",
    "\n",
    "answers = [\" 2\", \"2\", \"t bunch\"]\n",
    "fulls = [\"1 = 2\", \"1 = 2\", \"we are a neat bunch\"]\n",
    "ids = tokenizer(fulls, fulls, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "print(right_aligned_covering2(ids, answers, tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: tensor([[50256, 50256, 50256, 50256, 50256, 50256, 31373],\n",
      "        [50256,  1858,   338,   645,   640,   588,   262]])\n",
      "completions: tensor([[50256, 50256, 50256, 50256, 50256, 50256,   995],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256,  1944]])\n",
      "full: tensor([[50256, 50256, 50256, 50256, 50256, 31373,   995],\n",
      "        [ 1858,   338,   645,   640,   588,   262,  1944]])\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byoufools.ddns.net/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     sequence_nlls \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(token_nlls, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byoufools.ddns.net/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m sequence_nlls, token_nlls, outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Byoufools.ddns.net/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m sequence_nlls, token_nlls, loss \u001b[39m=\u001b[39m completion_likelihood(model, tokenizer, [\u001b[39m\"\u001b[39;49m\u001b[39mhello\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mThere\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms no time like the\u001b[39;49m\u001b[39m\"\u001b[39;49m], [\u001b[39m\"\u001b[39;49m\u001b[39m world\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m present\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "\u001b[1;32m/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byoufools.ddns.net/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m nlls \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mF\u001b[39m.\u001b[39mlog_softmax(outputs\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byoufools.ddns.net/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m shifted_labels \u001b[39m=\u001b[39m labels[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m:]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Byoufools.ddns.net/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m shifted_mask \u001b[39m=\u001b[39m mask_out[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m1\u001b[39;49m:]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byoufools.ddns.net/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m nlls \u001b[39m=\u001b[39m nlls[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byoufools.ddns.net/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m sel \u001b[39m=\u001b[39m shifted_labels[:, :, \u001b[39mNone\u001b[39;00m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def completion_likelihood(model, tokenizer, prompts, completions):\n",
    "    full = [p + c for p, c in zip(prompts, completions)]\n",
    "    tokenized_full = tokenizer(full, return_tensors=\"pt\", padding='longest')\n",
    "    num_tokens = tokenized_full.input_ids.shape[1]\n",
    "\n",
    "    tokenized_prompts = tokenizer(prompts, return_tensors=\"pt\", padding='max_length', max_length=num_tokens)\n",
    "    tokenized_completions = tokenizer(completions, return_tensors=\"pt\", padding='max_length', max_length=num_tokens)\n",
    "    mask_out = right_aligned_covering(tokenized_completions.input_ids, tokenized_full.input_ids, tokenizer.pad_token_id)\n",
    "    print(\"prompts:\", tokenized_prompts.input_ids)\n",
    "    print(\"completions:\", tokenized_completions.input_ids)\n",
    "    print(\"full:\", tokenized_full.input_ids)\n",
    "    print(mask_out)\n",
    "    labels = tokenized_full.input_ids.clone()\n",
    "    labels[mask_out] = -100\n",
    "\n",
    "    outputs = model(**tokenized_full, labels=labels)\n",
    "    nlls = -F.log_softmax(outputs.logits, dim=-1)\n",
    "    shifted_labels = labels[..., 1:]\n",
    "    shifted_mask = mask_out[..., 1:]\n",
    "    nlls = nlls[:, :-1, :]\n",
    "    sel = shifted_labels[:, :, None]\n",
    "    sel = torch.clamp(sel, min=0)\n",
    "\n",
    "    token_nlls = torch.gather(nlls, 2, sel)[:, :, 0]\n",
    "    token_nlls[shifted_mask] = 0\n",
    "    sequence_nlls = torch.mean(token_nlls, dim=1)\n",
    "    return sequence_nlls, token_nlls, outputs.loss\n",
    "\n",
    "sequence_nlls, token_nlls, loss = completion_likelihood(model, tokenizer, [\"hello\", \"There's no time like the\"], [\" world\", \" present\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2]) torch.Size([2, 6]) torch.Size([])\n",
      "tensor([2.6944, 1.7968], grad_fn=<MeanBackward1>) tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 10.8904,  5.2759],\n",
      "        [ 1.6358,  1.8183,  4.9683,  2.3118,  0.0328,  0.0140]],\n",
      "       grad_fn=<AsStridedBackward0>) tensor(3.3684, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sequence_nlls.shape, token_nlls.shape, loss.shape)\n",
    "print(sequence_nlls, token_nlls, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4254)\n",
      "tensor(4.1840)\n",
      "tensor(3.7333)\n",
      "tensor(3.1808)\n",
      "tensor([8.5288, 0.6429, 0.3706])\n"
     ]
    }
   ],
   "source": [
    "ls = torch.tensor(gt_ls)\n",
    "print(ls.mean())\n",
    "print(ls[1:].mean())\n",
    "print(ls[:-1].mean())\n",
    "print(ls[1:-1].mean())\n",
    "print(ls[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Byoufools.ddns.net/home/william/Workspaces/sequential_inference/experiments/inference_notebook.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m]) \u001b[39m==\u001b[39;49m torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m5\u001b[39;49m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 0"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
